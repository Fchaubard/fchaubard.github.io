<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>(Train Time) Recurrence as a necessary condition for General Intelligence
</title>
</head>
<body>
  <h1>(Train Time) Recurrence as a necessary condition for General Intelligence</h1>

  

<p> To get to AGI, we need an architecture that is "Turing Complete". And a necessary condition of an architecture to be turing complete is “unbounded recurrence/iterations”. The program needs to be able to call itself as many times as it wants. LLMs / diffusion models are said to be “Turing Complete” [<a href="https://medium.com/heyjobs-tech/turing-completeness-of-llms-d38b5e7bf795">1</a>]. However, this is <b>only</b> true at test time, NOT during training. Infinite recursion is not possible in one just foreward pass of the model as is done during training. To make this work at all, we need to teacher force explicitly the intermediary goal states (per time step) to achieve any amount of success (CoT for LLMs, and a bit denoised noisy input for diffusion). However, this is much more limiting than most people can comprehend. We train it this way and then cross our fingers and hope that the non-recurrent model can be called in while true loop and will be turing complete. Lets call this method teacher forcing recurrence. 
</p>
  
  <p> As is the most popular SOTA approach right now, what I call "teacher forcing recurrence" + Backprop, your task must be 1) decomposable into 'known' intermediary steps explicitly provided to the model (snapped into a discrete token space for LLMs, or in continuous pixel space for diffusion image gen), and 2) you need to train it starting from one 'known' intermediary results to another 'known' intermediary result with just one fpass to the model. If both are not true, more inference, model size, tokens, training steps, gpus, etc will provide no more value to you. And it wont get us to AGI. e.g. if its a problem where we dont know the intermediary results/decomposition? or worse we dont even know the final answer? or, the intermediary results are not decomposable into "discrete tokens" (e.g. visual simulation is better than text toks)? in all cases this "smearing" over token space wont save you, no matter how much you train or test time infer. Thinking of Ramanujan's "god gave me the answer" where he couldnt explain where the answer came from. How to train it to think like this? How to solve any of the millinium prize problems? Whats the trace? To get the rest of the way there, we need to make distinct problem classes that highlight these issues with current architectures / training techniques so we can try out new ones on tiny subproblems per class and once we get some working, then worry about scale up. Like learning simple things like rolling sum, sin, sort, copy, DFS, BFS, factorial, all the CLRS algos, etc etc.. non-compressible sequential problems. </p>

  <p> I have heard specious arguments that RL against 'enough' of these traces will get you there and it will somehow learn the recurrence programs even beyond what is was trained on. E.g. train on more lean proofs and it will learn how to solve unsolved problems. While there may be examples of it working using the "same program we already had it learn" it will never learn a new way to prove theorems. Furthermore, this is NOT a path to intelligence. Intelligence was not created on earth by training on a bunch of lean proofs or code.. In fact the opposite! The lean proofs were created BY THE INTELLIGENCE! So what process/training procedure/architecture created the intelligence? </p>

  <p>
    You need to let the model recurse as many times as it wants <b>during training</b> to enable new cool programs to be found, and you should not force it to do so in only one fpass, or tell it what each intermediary state should be so explicitly. This is just too limiting. When you release this constrainst, you let the model find super cool novel hierarchies and programs that we have never thought of! And even better the programs could end up being really simple, finding a very small Kolmogorov Complexity solution. Think for example if we only know about bubble sort and not merge sort for example. Merge sort is just a few if statements and a recursive call. Each recursion loop being one level of heirarchy in the 'tree' of compute. Quite simple but powerful. Instead teacher forcing on bubble sort will be complicated and awful at run time and will not get us to merge sort. More generally, by pinning to "current SOTA solutions" we will never get to "beyond SOTA solutions". A clearer way to describe this is you must enable "adaptive compute time in latent space" (could be continuous latents or a codebook, either way). And IMO is a necessary architecture feature to get the rest of the way there to AGI/ASI. 
  </p>
  
  <p> Another way to look at this is "self-boosting". We let the model learn to improve its own input by giving in previous outputs of itself and try to improve upon it. This is using the correct input to the model in both cases as at test time, this is what will really happen, and is the inductive bias we are looking for. We need recurrence <i>embedded</i> into the architecture itself and train it to be a turing machine, not turn a Non-turing machine into a turing machine with a white true loop. 
</p> 
<p> Our brain is massively ‘loopy’ / recurrent and that is embedded into the architecture itself, in both “inner loops” (microcircuits) and big “outer loops” (2 lobes). This is not a random detail that we can circumnavigate and still achieve nice things. Going “bigger” will not solve this (perhaps theoretically, but not realistically).  
</p>
<p> We just need the algo to train it. 
</p>

<p>
  What are some candidates to enable this train time recurrence? 
</p>
  <ol>
    <li>HRM: Some magic has been proven with this paper. They can do TBPTT with T=1 and somehow nice things happen. Can't believe it. Very similar to my SLN (Strange Loop Networks actually). There must be a few scaling laws with this paper. Scaling in Nmax. Scaling in number of frequencies maybe? Scaling in model size. Etc. The only thing I dont like about this paper is there is no "search" at test time. You want that embedded into the architecture. Side note: If TBPTT @ T=1 works for reasoning, it should work for diffusion too. Diffusion only iterates once on theta before incurring loss. Perhaps, we should enable do the same training procedure but allow N ponder steps sampled between Nmin and Nmax for diffusion/flow matching as well! Training not from random noised up images but from the biased sampling of the model. never heard of training image/video gen like that. Should work! </li>
    <li>Zero Order: like SPSA or otherwise. </li>
    <li>Mix of both?</li>
  </ol>
<p> Conclusion: Stacked Transformers + CoT scaling test-time tokens is just not the right inductive bias: they still struggle to remember what matters which is why they dont work well in long horizon tasks, they can not perform induction, and despite all that, they require a disgusting amount of memory/compute. We need to refocus toward architectures with embedded recurrence. This is the cleanest path to reliable AGI. 
</p>
  




    <h3><a href="./index.html">Return Home</a></h3>
</body>
</html>
