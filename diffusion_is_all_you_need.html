<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Is Diffusion All You Need? Is Diffusion (Langevin Dynamics) the Free Lunch We Have All Been Waiting For?</title>
</head>
<body>
    <h1>Is Diffusion All You Need? Is Diffusion (Langevin Dynamics) the Free Lunch We Have All Been Waiting For?</h1>
    
    <p>Every month that passes, I grow more convinced that Diffusion is the free lunch we have all been searching for in AI. Its dominating subfield after subfield. It works so well because diffusion models can model multimodal distributions effectively, scales naturally to high-dimensional action spaces, and provide incredible training stability.. as many who have tried them say "It just works". We’ve seen this play out in multiple domains. For example, GNNs were SOTA for protein folding and weather prediction (GraphCast) but last year got dethroned by GenCast (diffusion-based) <a href="https://arxiv.org/abs/2311.13262">[3]</a>. They have already dominated Image Generation 3 years ago, Video Generation 2 years ago, and then last year Diffusion Policy dominated robotics: <a href="https://diffusion-policy.cs.columbia.edu/">[1]</a> which IMO is a bigger deal than AlexNet! Big claim I know. But just read this from their abstract: </p> 
    <blockquote>
        "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%."
    </blockquote>
    
    <p>AlexNet dropped ImageNet error from ~30% to ~10%, but Diffusion Policy jumped robotics task success rates from ~20% to ~80%—this is a much bigger leap! This is the AlexNet moment for diffusion (Langevin dynamics) training IMO and the AI community is not really talking about it. And this week, LLADA <a href="https://arxiv.org/abs/2502.09992">[2]</a> just got published and gets comparable performance to AR-based LLMs! Moreso, there has been no "Grad Student Descent" or nowadays "VC descent" on it yet. Just wait. Does the rest of the AI community not see it yet? Just a matter of time until its used to solve GO, poker, MARL, and other hold out subfields where diffusion is not SOTA yet. </p>
    
    <h2>The Squint Test: Why Diffusion Feels More "Right" than AR</h2>
    
    <p>Here’s my <strong>squint test</strong>: If I squint at a plane and a bird, I can see how they’re similar—good. If I squint at an architecture with stacked transformers and 3 different training stages, and compare it to the brain with two giant lobes with a huge fissure on inhibition separating them, with continuous learning / single objective function the entire time on remarkably small amounts of data resulting in huge generalization, it <strong>does NOT</strong> pass my squint test. The biggest tell IMO actually? <strong>Time to first token.</strong></p>
    
    <p>Humans don’t just start spewing out words instantly. We ponder. We ponder more for harder problems naturally and less for less. We engage in a kind of iterative refinement before speaking. AR-LLMs don’t do this; they predict one token at a time, with fixed compute for each token, and iter time per token (generally). I know test time compute means just let it "think" for longer and that is true. But this is not the same thing. There is a hard snapping back to token space which may not be right given the problem. While it may be the same thing in the end to "smear the compute" in the time domain, but for some tasks, you do not have a COT trace, or if you mandate the model to not output anyhting but the answer, this won't work. E.g. bit parity / rolling sum without COT. Diffusion allows a more natural process for it to keep calling the model again and again until convergence which is a very natural convergence compared to AR-LLMs that must output an "eos" token. In diffusion, w/ more inference steps, nonconvergence of the output == more “thinking” is needed. Very natural. And even more important IMO, that thinking is NOT broadcasted back to token space, it remains in latent space. WHich if you think about how you ponder, you aren't necessarily thinking in token space, maybe you are visualizing, maybe you aren't even aware of what space your are doing your thinking in. That is what diffusion models are doing. Depending on implementation, there isn't necessarily a forcing back to token space. You can stay in latent until you feel like you are "done thinking". And also, its natural to think about type 1 vs. type 2 thinking. Type 1 is just first iter on the diffusion at inference time. Type 2, is do it for longer. This aligns with how humans do type 1/2 to me!</p>
    
    <h2>LLADA: Diffusion Is Already Competitive with AR-LLMs</h2>
    
    <p>We’re already seeing diffusion models hold their own against AR-LLMs. Enter <a href="https://ml-gsai.github.io/LLaDA-demo/">LLADA</a>: <a href="https://arxiv.org/abs/2502.09992">[2]</a>.</p>
    
    <blockquote>
        "LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task."
    </blockquote>
    
    <p>AR-LLMs assume we think by computing X FLOPS, emitting one token, then computing X FLOPS again for the next. But we don’t! We do a lot of upfront thinking and then generate structured output <strong>quickly</strong>. Diffusion allows that—more inference steps mean deeper pondering, leading to higher-quality generations.</p>
    
    <h2>Vibe Check and is it biologically plausible?</h2>
    
    <p>I really hate that term but I can't think of a better header. When I talk to everyone who has tried diffusion training for their problem every says the same thing.. "it just works". Like no HPP tuning. That is nuts! The other vibe check is diffusion itself is very easy to see how the brain could be doing it. Diffusion is literally one of the most basic processes in physics. Its seen everywhere. And in the brain it obviously appears as well. You have path one which is a "clean" path and then you have a "dirty path" which is any other circuit that has a few more neurons in the way and then you just want to get the dirty path to match the clean. To "denoise" the clean. Thats it. I dont pretend to be a neuroscientist but it seems obvious. </p>
    
    <h2>Diffusion as a General-Purpose Training Method</h2>
    
    <p>Diffusion models are not just for generative tasks; they can be adapted for unsupervised, supervised, and reinforcement learning (RL). Here’s a breakdown:</p>
    
    <h3>Unsupervised Learning</h3>
    <p>Diffusion models inherently learn data distributions by denoising. This makes them incredibly effective for unsupervised learning tasks like density estimation and self-supervised representation learning.</p>
    
    <h3>Supervised Learning</h3>
    <p>By conditioning the reverse diffusion process on labels, diffusion models can be adapted to perform supervised classification or regression.</p>
    
    <h3>Reinforcement Learning</h3>
    <p>Diffusion-based policies, such as in Diffusion Policy, learn to generate optimal action sequences by modeling the underlying policy as a distribution over possible actions. This improves multimodal decision-making and stability.</p>
    
    <h2>Pseudo Code for Diffusion-Based Learning</h2>
    <p> The last argument I will make is occam's razor. Diffusion is so so so simple! Just look at this. It could be used for supervised learning, unsupervised learning, RL, etc. And it always "just works"! </p>
    
    
    <pre>
    def diffusion_train(data, num_timesteps):
        for (x, y) in data:
            noise = sample_gaussian_noise()
            y_hat = x  # Initialize prediction sequence
            for t in range(num_timesteps):
                y_hat = apply_noise(y_hat, noise, t)
                loss = model.train(y_hat, y)  # Try to get y_hat closer to y
    
    def diffusion_inference(x, num_timesteps):
        y_hat = x  # Start from noisy input
        for t in reversed(range(num_timesteps)):
            y_hat = model.denoise_step(y_hat, t)
        return y_hat
    </pre>
    
    <h2>Conclusion: Diffusion Is the Free Lunch</h2>
    
    <p>Diffusion models are hitting SOTA across so many domains, and I believe they <strong>are</strong> the free lunch we’ve all been waiting for. This is still early, but the results are already insane. Stay tuned!</p>
    
    <hr>
    
    <h2>References</h2>
    
    <ol>
        <li id="ref1">Chi, H., Li, L., &amp; Wang, B. (2023). <em>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</em>. Retrieved from <a href="https://diffusion-policy.cs.columbia.edu/">Diffusion Policy Project</a>.</li>
        <li id="ref2">GSAI Team (2025). <em>LLaDA: Scaling Diffusion Models for Language Tasks</em>. <a href="https://arxiv.org/abs/2502.09992">arXiv:2502.09992</a>.</li>
        <li id="ref3">Pathak, A. et al. (2023). <em>GenCast: Diffusion-Based Weather Prediction</em>. <a href="https://arxiv.org/abs/2311.13262">arXiv:2311.13262</a>.</li>
        <li id="ref4">Langevin, P. (1908). <em>On the Theory of Brownian Motion</em>. Comptes Rendus de l'Académie des Sciences.</li>
    </ol>
    
    <hr>
    <h3><a href="./index.html">Return Home</a></h3>
</body>
</html>
