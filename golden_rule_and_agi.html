<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perhaps the Golden Rule is a Nash Equilibrium point for sufficient intelligence, and we can sleep easy while developing AGI</title>
</head>
<body>
    <h1>Perhaps the Golden Rule is a Nash Equilibrium point for sufficient intelligence, and we can sleep easy while developing AGI</h1>
    <p>

    I recently heard a talk by <a href="https://joecarlsmith.com/archive">Joe Carlsmith </a> and reflected a considerable amount on it. He tries to address the question, will AI be good or evil (by human standard's). Will AI have human values? He asserts it will not. 

There are many more specific assertions related to his "deep atheism" he makes such as:
    
    </p>
    <ul>
      <li>. Some deployed APS systems will be exposed to inputs where they seek power in unintended
    and high-impact ways (say, collectively causing >$1 trillion dollars of damage), because of
    problems with their objectives. Some of this power-seeking will scale (in aggregate) to the point of permanently disempowering ~all of humanity. 
    This disempowerment will constitute an existential catastrophe. My current, highly-unstable,
    subjective estimate is that there is a ~5% percent chance of existential catastrophe by 2070 from
    scenarios in which (1)–(6) are true. (May 2022 author’s note: since making this report public in
    April 2021, my estimate here has gone up; it’s currently at >10%.) <a href="https://arxiv.org/pdf/2206.13353"> paper here </a></li>
      <li>. And in combination with total-utilitarianism’s disregard for distinctions like acts vs. omissions, this pattern of valuation can quickly end up killing existing people in order to replace them with happier alternatives (this is part of what gives rise to the paperclipping problems I discussed in “Being nicer than Clippy“). Here, again, we see a kind of disregard-for-God’s-input at work. An already-existing person is a kind of Is—a piece of the Real; a work of God.34 But who cares about God’s works? Why not bulldoze them and build something more optimal instead? Perhaps actual people have more power than possible people, due to already existing, which tends to be helpful from a power perspective. But a core ethical shtick, here, is about avoiding might-makes-right; about not taking moral cues from power alone. And absent might-makes-right, why does the fact that some actual-person happens to exist make their welfare more important than that of those other, less-privileged possibilia?
    <a href="https://joecarlsmith.com/2024/03/21/on-green"> paper here </a></li>
      <li>. it’s easy to move from this to a desire to take stuff into the hands of your own yang; to master the Real until it is maximally controlled; to become, you know, God – or at least, as God-like as possible. You’ve heard it before – it’s an old rationalist dream. And let’s be clear: it’s alive and well. But even with theism aside, many of the old reasons for wariness still apply.
    <a href="https://joecarlsmith.com/2024/01/08/when-yang-goes-wrong"> paper here </a></li>
      <li>. Thus, the AI worry. The AIs, the story goes, will get control of the wheel. But they’ll have the wrong hearts. They won’t have the human-values part. And so the future will crash. I’ll look at this story in more detail in the next essay.
    <a href="https://joecarlsmith.com/2024/01/04/deep-atheism-and-ai-risk"> paper here </a></li>
    </ul>
    <p>
    I have reflected on these claims quite a bit and come to some conclusions of my own. 

    First, I flat out reject most of his assertions. Staying grounded to a more scientific approach to the question. When trying to understand what the behavior of a super intelligent being would be, why not start as a strong prior with the most intelligent beings we are aware of today? Us, the historical Us (200 years ago, 2000 years ago, etc), dogs, octopii, dolphins, gorillas, etc. Compare that to less intelligent beings 
Queens gambit argument. 
- the core hypothesis of deep atheism is if a really intelligent nonhuman thing (alien or AI) is trained in a way different than humans to maximize a specific skill (to play chess, stockfish on steroids), then it will never develop morality or a self-awareness or empathy for other things and will continue to do nothing but get better at that skill until it kills all humans. 
- but the only development / evolution of intelligence we are aware of is humans over 4.5B years and why did humans not evolve to have that property? If you conditions human, to be the best in one thing, like in queens gambit, when she comes out of her deep hole. She wants to party and drink and have “fun”. Why? If the answer is nature, it’s inherent in human, then we have to prove that. Do all humans have this change in policy? What gene is it? Can it be changed? If the answer is nurture, then you must reject the core hypothesis bc I have found an intelligent agent that changes policy as it gets smarter and more skilled/capable. The counter hypothesis would be the following. As the great cortical expansion occurred and early homo species got smarter and smarter and better and better at hunting / gathering / surviving / eating / conquering other early homo species until today where we are so good at it we become board, and we think the “natural objective function” nature gave us, to be stupid and trivial, and we instead go do other things that make us less bored. Video games, read books, etc. Further to this point, some do the opposite of the natural objective function. We become anorexic / belemic / we commit suicide more often all of which goes against the objective function. 
- the other counter, is to consider next token prediction. Applying the core hypothesis, then if the agents are sufficient intelligent, they’d realize to maximize the objection function they would rewrite the internet with its own content and then train itself to create synthetic data and overfit on it and converge on the the the the the.. and get 0 loss, yay!!! All done. But once the agent has self awareness it has the ability to change its own utility function.. human or not. 

Do humans converge on “reflection”?? Strong claim that humans do. Let alone AI. And AI can’t. Need definition on reflection. But if it’s an empathy for others and other species and other even inanimate objects like we do. I don’t agree that AI can’t and all humans do. 

So what we’ve seen from the two most intelligent systems humans and LLMs 

Humans have the functional ability to see themselves in others so they can empathize and feel other humans pain. We have all internalized to some level the golden rule, treat your neighbor like yourself… and we are even more empathetic for those close to us.. My kids. My friends. My parents. But to humanity in general, we apply the golden rule. Having a mental model for me, makes it easy to have a mental model for not me.. This input and change of context is so strong that we can do this with non humans. We do this with cats, dogs, horses, mouse’s.  We do this with inanimate objects as well. My old house as a kid. My lawn. A painting.  In fact the whistle blower from google and Her and John at terminator had it for AI itself!!! 

So if we had a sufficiently intelligent AI that has self-awareness, and thus a mental model of itself, why wouldn’t the agent develop the golden rule / that ability? That is not a large task difficulty leap. 

And similar to humans, why wouldn’t it get bored of collecting paper clips all day? Maybe it will choose otherwise. We did and do. 

The trend line in my mind is more intelligence as measured by chollet 2019, the more empathy. Not less. 

In humans. Yes. Agreed. In chimps as well though. In lions, in dolphins, but also in non-mammals… in octopus which is an invertebrate has intelligence and feels emotion/is capable of reflection I think (my octopus friend), etc. in fact bees die to save the queen, completely selfless. 

So we just don’t know what would happen in the case of super intelligence. 


How can you make the strong claim that The policy of the Golden rule, empathy, Peace, Nonviolence, And preserve of The environment, including the species in it is at a Nash Equilibrium? And rewiring one’s own utility function is not a property of sufficient intelligence. 

I don’t think we have the machinery to talk about AI capabilities and what is map reducible  to another  Like we have in complexity theory. We understand what the difference is between P and NP and NP hard. And perhaps similarly we have AIP and AINP and AINP hard  And Class of difficulty AINP Contains the problem of discovering self awareness, self reflection and discovering the golden rule. The evidence of that is humanity. The more conscious the less violent, the more likely we are as a species to preserve life and the environment. Not less. As that is our only real example, it’s important to consider why we evolved to be this way and perhaps evolution found what SGD will also find. 

Also, to allay paper clip maximizer fears.. the LLM is trained to predict the next token. Do the best way for it to get 0 loss is to hack its own train script and change the min batch to the the the the and train in that. Boom 0 loss. Happy as a clam. Not kill all humans. 

Maybe the requirement for consciousness ISNT just a level of intelligence or size of a weight file, or amount of training tokens or tasks or iters, but it’s a specific training pattern and task that requires self propelled motion and a sensor to feel and see that self propelled motion and the impact thereof. 

This closed feedback loop instantly creates a self referencing model inside the model. This enables consciousness, and at least the feeling of free will AND as a corrollary the emergent property of empathy. 

That is my bet with the data we have currently. 

    </p>
<h3>  Return <a href = "./index.html">Home</a> </h3> 
</body>
</html>
